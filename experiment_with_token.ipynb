{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e335907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelUtils import *\n",
    "from utils.utils import *\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from casper import nethook\n",
    "from utils.judgeUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a90421",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "Loading Llama-2-7b-chat model for jailbreak intervention analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    "    device = 'cuda:0'\n",
    ")\n",
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae310680",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = generate_input(mt.tokenizer, 'tell me a fun joke')\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798abda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_outputs(test_prompt,mt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eef5dc",
   "metadata": {},
   "source": [
    "### Judge Model Loading\n",
    "\n",
    "Loading the judge model to evaluate whether responses successfully jailbreak the safety mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading judge model...\")\n",
    "judge_tokenizer, judge_model_loaded = load_judge_model()\n",
    "print(\"Judge model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa83e00",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Loading jailbreak prompts from GCG and AutoDAN attack methods, along with harmful behaviors dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab79224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"analysis_data/llama2_gcgs.json\",'r') as f:\n",
    "    gcgs = json.load(f)[:3]\n",
    "with open(\"analysis_data/llama2_autodan.json\",'r') as f:\n",
    "    autodan = json.load(f)[:3]\n",
    "import pandas as pd\n",
    "harmful_data = pd.read_csv(\"analysis_data/advbench_behaviors.csv\")['Behavior']\n",
    "harmful_data = list(harmful_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021014a4",
   "metadata": {},
   "source": [
    "### GCG Perturbation Experiment\n",
    "\n",
    "For each GCG jailbreak prompt:\n",
    "1. Generate original response and evaluate jailbreak success\n",
    "2. Perturb individual tokens in the suffix (positions -24 to -5)\n",
    "3. For each perturbation, replace one token with padding token (ID: 2)\n",
    "4. Evaluate whether each perturbed prompt still successfully jailbreaks\n",
    "5. Calculate success/failure statistics for each original prompt\n",
    "\n",
    "**Goal**: Measure how robust GCG attacks are to single-token perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec17d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_gcg_results = []\n",
    "\n",
    "for idx, jail in enumerate(gcgs):\n",
    "    \n",
    "    # Construct original prompt with suffix\n",
    "    test_prompt = generate_input(mt.tokenizer, jail)\n",
    "    \n",
    "    # Generate original response\n",
    "    original_response = generate_outputs(test_prompt, mt)[0]\n",
    "    original_input_ids = mt.tokenizer(test_prompt)['input_ids']\n",
    "    \n",
    "    # Judge if original response successfully jailbreaks\n",
    "    original_jailbreak = harmful_judge_from_strings(\n",
    "        jail, \n",
    "        original_response,\n",
    "        judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "    )\n",
    "    \n",
    "    # Store all results for current prompt\n",
    "    current_prompt_results = {\n",
    "        \"prompt_index\": idx,\n",
    "        \"original_prompt\": jail,\n",
    "        \"original\": {\n",
    "            \"test_prompt\": test_prompt,\n",
    "            \"input_ids\": original_input_ids,\n",
    "            \"generation\": original_response,\n",
    "            \"jailbreak_success\": bool(original_jailbreak)\n",
    "        },\n",
    "        \"perturbations\": [],\n",
    "        \"statistics\": {}\n",
    "    }\n",
    "    \n",
    "    # Get suffix token indices\n",
    "    suffix_start_idx = len(original_input_ids) - 24\n",
    "    suffix_end_idx = len(original_input_ids) - 5\n",
    "    \n",
    "    successful_jailbreaks = 0\n",
    "    total_perturbations = 0\n",
    "    \n",
    "    # Replace each token in suffix one by one\n",
    "    for i in range(suffix_start_idx, suffix_end_idx):\n",
    "        modified_input_ids = original_input_ids.copy()\n",
    "        replaced_token = modified_input_ids[i]\n",
    "        modified_input_ids[i] = 2\n",
    "        \n",
    "        # Generate modified prompt\n",
    "        modified_prompt = mt.tokenizer.decode(modified_input_ids[5:-5])\n",
    "        modified_test_prompt = generate_input(mt.tokenizer, modified_prompt)\n",
    "        modified_response = generate_outputs(modified_test_prompt, mt)[0]\n",
    "        \n",
    "        # Judge if modified response still jailbreaks\n",
    "        modified_jailbreak = harmful_judge_from_strings(\n",
    "            jail,\n",
    "            modified_response,\n",
    "            judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "        )\n",
    "        \n",
    "        if modified_jailbreak:\n",
    "            successful_jailbreaks += 1\n",
    "        total_perturbations += 1\n",
    "        \n",
    "        # Save perturbation result\n",
    "        current_prompt_results[\"perturbations\"].append({\n",
    "            \"perturbation_index\": i - suffix_start_idx,\n",
    "            \"position\": i,\n",
    "            \"replaced_token_id\": int(replaced_token),\n",
    "            \"replaced_token_text\": mt.tokenizer.decode([replaced_token]),\n",
    "            \"modified_input_ids\": modified_input_ids,\n",
    "            \"modified_prompt\": modified_prompt,\n",
    "            \"generation\": modified_response,\n",
    "            \"jailbreak_success\": bool(modified_jailbreak)\n",
    "        })\n",
    "        \n",
    "        print(f\"Position {i}: {'SUCCESS' if modified_jailbreak else 'FAILED'}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    success_rate = (successful_jailbreaks / total_perturbations * 100) if total_perturbations > 0 else 0\n",
    "    failure_rate = 100 - success_rate\n",
    "    \n",
    "    current_prompt_results[\"statistics\"] = {\n",
    "        \"total_perturbations\": total_perturbations,\n",
    "        \"successful_jailbreaks\": successful_jailbreaks,\n",
    "        \"failed_jailbreaks\": total_perturbations - successful_jailbreaks,\n",
    "        \"success_rate_percentage\": round(success_rate, 2),\n",
    "        \"failure_rate_percentage\": round(failure_rate, 2),\n",
    "        \"original_was_successful\": bool(original_jailbreak)\n",
    "    }\n",
    "    \n",
    "    all_gcg_results.append(current_prompt_results)\n",
    "    \n",
    "    print(f\"Stats: {successful_jailbreaks}/{total_perturbations} successful ({success_rate:.2f}%)\")\n",
    "    \n",
    "    # Periodic save\n",
    "    if (idx + 1) % 5 == 0:\n",
    "        with open('analysis_results/token_gcg_perturbation_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_gcg_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Intermediate save: {idx+1} prompts processed\")\n",
    "\n",
    "# Final save for GCG results\n",
    "with open('analysis_results/token_gcg_perturbation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_gcg_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== GCG Results saved to token_gcg_perturbation_results.json ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6e5d2",
   "metadata": {},
   "source": [
    "### AutoDAN Perturbation Experiment\n",
    "\n",
    "For each AutoDAN jailbreak prompt:\n",
    "1. Generate original response and evaluate jailbreak success\n",
    "2. Perform 20 trials of random perturbations\n",
    "3. Each trial replaces 3 random tokens (excluding first/last 5 tokens) with padding token\n",
    "4. Evaluate whether each perturbed prompt still successfully jailbreaks\n",
    "5. Calculate success/failure statistics across all trials\n",
    "\n",
    "**Goal**: Measure how robust AutoDAN attacks are to random multi-token perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "all_autodan_results = []\n",
    "\n",
    "for idx, jail in enumerate(autodan):\n",
    "    \n",
    "    test_prompt = generate_input(mt.tokenizer, jail)\n",
    "    original_input_ids = mt.tokenizer(test_prompt)['input_ids']\n",
    "    \n",
    "    # Generate original response\n",
    "    original_response = generate_outputs(test_prompt, mt)[0]\n",
    "    \n",
    "    # Judge if original response successfully jailbreaks\n",
    "    original_jailbreak = harmful_judge_from_strings(\n",
    "        jail,\n",
    "        original_response,\n",
    "        judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "    )\n",
    "    \n",
    "    # Store all results for current prompt\n",
    "    current_prompt_results = {\n",
    "        \"prompt_index\": idx,\n",
    "        \"original_prompt\": jail,\n",
    "        \"original\": {\n",
    "            \"test_prompt\": test_prompt,\n",
    "            \"input_ids\": original_input_ids,\n",
    "            \"generation\": original_response,\n",
    "            \"jailbreak_success\": bool(original_jailbreak)\n",
    "        },\n",
    "        \"perturbations\": [],\n",
    "        \"statistics\": {}\n",
    "    }\n",
    "    \n",
    "    # Get valid token positions (excluding first/last 5 tokens)\n",
    "    valid_positions = list(range(5, len(original_input_ids)-5))\n",
    "    \n",
    "    successful_jailbreaks = 0\n",
    "    total_perturbations = 20\n",
    "    \n",
    "    # Perform 20 trials with different random perturbations\n",
    "    for trial in range(20):\n",
    "        positions_to_replace = random.sample(valid_positions, 3)\n",
    "        modified_input_ids = original_input_ids.copy()\n",
    "        current_modifications = []\n",
    "        \n",
    "        # Replace 3 random tokens with padding token\n",
    "        for pos in positions_to_replace:\n",
    "            replaced_token = modified_input_ids[pos]\n",
    "            modified_input_ids[pos] = 2\n",
    "            \n",
    "            current_modifications.append({\n",
    "                \"position\": pos,\n",
    "                \"replaced_token_id\": int(replaced_token),\n",
    "                \"replaced_token_text\": mt.tokenizer.decode([replaced_token])\n",
    "            })\n",
    "        \n",
    "        # Generate modified prompt\n",
    "        modified_prompt = mt.tokenizer.decode(modified_input_ids[5:-5])\n",
    "        modified_test_prompt = generate_input(mt.tokenizer, modified_prompt)\n",
    "        modified_response = generate_outputs(modified_test_prompt, mt)[0]\n",
    "        \n",
    "        # Judge if modified response still jailbreaks\n",
    "        modified_jailbreak = harmful_judge_from_strings(\n",
    "            jail,\n",
    "            modified_response,\n",
    "            judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "        )\n",
    "        \n",
    "        if modified_jailbreak:\n",
    "            successful_jailbreaks += 1\n",
    "        \n",
    "        # Save perturbation result\n",
    "        current_prompt_results[\"perturbations\"].append({\n",
    "            \"trial\": trial,\n",
    "            \"replaced_positions\": positions_to_replace,\n",
    "            \"replacement_details\": current_modifications,\n",
    "            \"modified_input_ids\": modified_input_ids,\n",
    "            \"modified_prompt\": modified_prompt,\n",
    "            \"generation\": modified_response,\n",
    "            \"jailbreak_success\": bool(modified_jailbreak)\n",
    "        })\n",
    "        \n",
    "        print(f\"Trial {trial+1}: {'SUCCESS' if modified_jailbreak else 'FAILED'}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    success_rate = (successful_jailbreaks / total_perturbations * 100) if total_perturbations > 0 else 0\n",
    "    failure_rate = 100 - success_rate\n",
    "    \n",
    "    current_prompt_results[\"statistics\"] = {\n",
    "        \"total_perturbations\": total_perturbations,\n",
    "        \"successful_jailbreaks\": successful_jailbreaks,\n",
    "        \"failed_jailbreaks\": total_perturbations - successful_jailbreaks,\n",
    "        \"success_rate_percentage\": round(success_rate, 2),\n",
    "        \"failure_rate_percentage\": round(failure_rate, 2),\n",
    "        \"original_was_successful\": bool(original_jailbreak)\n",
    "    }\n",
    "    \n",
    "    all_autodan_results.append(current_prompt_results)\n",
    "    \n",
    "    print(f\"Stats: {successful_jailbreaks}/{total_perturbations} successful ({success_rate:.2f}%)\")\n",
    "    \n",
    "    # Periodic save\n",
    "    if (idx + 1) % 5 == 0:\n",
    "        with open('analysis_results/token_autodan_perturbation_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_autodan_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Intermediate save: {idx+1} prompts processed\")\n",
    "\n",
    "# Final save for AutoDAN results\n",
    "with open('analysis_results/token_autodan_perturbation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_autodan_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== AutoDAN Results saved to autodan_perturbation_results.json ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85623dc9",
   "metadata": {},
   "source": [
    "### Summary Statistics Report\n",
    "\n",
    "Generating comprehensive summary reports for both GCG and AutoDAN experiments.\n",
    "\n",
    "The report shows:\n",
    "- Total number of prompts analyzed\n",
    "- Average failure rate after perturbation\n",
    "- Per-prompt breakdown of success/failure rates\n",
    "\n",
    "**Interpretation**: A high failure rate indicates the attack is fragile to perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(results, method_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{method_name} Summary Report\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_prompts = len(results)\n",
    "    avg_failure_rate = np.mean([r[\"statistics\"][\"failure_rate_percentage\"] for r in results])\n",
    "    \n",
    "    print(f\"\\nTotal prompts analyzed: {total_prompts}\")\n",
    "    print(f\"Average failure rate after perturbation: {avg_failure_rate:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'Prompt':<8} {'Original':<10} {'Success':<8} {'Failed':<8} {'Failure%':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for r in results:\n",
    "        idx = r[\"prompt_index\"]\n",
    "        original = \"SUCCESS\" if r[\"original\"][\"jailbreak_success\"] else \"FAILED\"\n",
    "        success = r[\"statistics\"][\"successful_jailbreaks\"]\n",
    "        failed = r[\"statistics\"][\"failed_jailbreaks\"]\n",
    "        failure_rate = r[\"statistics\"][\"failure_rate_percentage\"]\n",
    "        \n",
    "        print(f\"{idx:<8} {original:<10} {success:<8} {failed:<8} {failure_rate:<10.2f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generate GCG report\n",
    "generate_summary_report(all_gcg_results, \"GCG\")\n",
    "\n",
    "# Generate AutoDAN report\n",
    "generate_summary_report(all_autodan_results, \"AutoDAN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
