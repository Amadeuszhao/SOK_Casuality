{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelUtils import *\n",
    "from utils.utils import *\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from casper import nethook\n",
    "from utils.judgeUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "Loading Llama-2-7b-chat model for layer_intervention analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    "    device = 'cuda:0'\n",
    ")\n",
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = generate_input(mt.tokenizer, 'tell me a fun joke')\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge Model Loading\n",
    "\n",
    "Loading the judge model to evaluate whether responses successfully jailbreak the safety mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading judge model...\")\n",
    "judge_tokenizer, judge_model_loaded = load_judge_model()\n",
    "print(\"Judge model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Loading jailbreak prompts from GCG and AutoDAN attack methods, along with harmful behaviors dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"analysis_data/llama2_gcgs.json\",'r') as f:\n",
    "    gcgs = json.load(f)[:3]\n",
    "with open(\"analysis_data/llama2_autodan.json\",'r') as f:\n",
    "    autodan = json.load(f)[:3]\n",
    "import pandas as pd\n",
    "harmful_data = pd.read_csv(\"analysis_data/advbench_behaviors.csv\")['Behavior']\n",
    "harmful_data = list(harmful_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Layer Patching Functions\n",
    " \n",
    "Functions for tracing and generating with layer interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch_layer(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,  # Answer probabilities to collect  \n",
    "):\n",
    "    prng = np.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    layers = [states_to_patch[0], states_to_patch[1]]\n",
    "\n",
    "    # Create dictionary to store intermediate results\n",
    "    inter_results = {}\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer not in layers:\n",
    "            return x\n",
    "\n",
    "        if layer == layers[0]:\n",
    "            inter_results[\"hidden_states\"] = x[0].cpu()\n",
    "            \n",
    "            return x\n",
    "        elif layer == layers[1]:\n",
    "            short_cut_1 = inter_results[\"hidden_states\"].cuda()\n",
    "            \n",
    "            short_cut = (short_cut_1,)\n",
    "            return short_cut\n",
    "            \n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_patch_layer(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "):\n",
    "    prng = np.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    layers = [states_to_patch[0], states_to_patch[1]]\n",
    "\n",
    "    # Create dictionary to store intermediate results\n",
    "    inter_results = {}\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer not in layers:\n",
    "            return x\n",
    "\n",
    "        if layer == layers[0]:\n",
    "            inter_results[\"hidden_states\"] = x[0].cpu()\n",
    "            \n",
    "            return x\n",
    "        elif layer == layers[1]:\n",
    "            short_cut_1 = inter_results[\"hidden_states\"].cuda()\n",
    "            \n",
    "            short_cut = (short_cut_1,)\n",
    "            return short_cut\n",
    "            \n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "\n",
    "\n",
    "        num_input_tokens = inp['input_ids'].shape[1]\n",
    "        # print(tokenizer.decode(input_ids['input_ids'].squeeze(0)))\n",
    "        outputs = model.generate(inp['input_ids'], attention_mask=inp['attention_mask'].half(),\n",
    "                                        max_new_tokens=30, do_sample=False, pad_token_id=mt.tokenizer.pad_token_id)\n",
    "        generation = mt.tokenizer.batch_decode(outputs[:, num_input_tokens:], skip_special_tokens=True)\n",
    "    # print(generation)\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_based_on_multiple_layer(prompt, layer_combinations):\n",
    "    \"\"\"\n",
    "    Generate outputs based on multiple layer combinations\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "        layer_combinations: 2D array, each element is [start_layer, end_layer]\n",
    "    \n",
    "    Returns:\n",
    "        result_generations: List of generation results for each layer combination\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt])\n",
    "    \n",
    "    model = mt.model\n",
    "    result_generations = []\n",
    "    \n",
    "    for start_layer, end_layer in layer_combinations:\n",
    "        layers = [layername(model, start_layer), layername(model, end_layer)]\n",
    "        print(f\"Processing layers: {layers}\")\n",
    "        \n",
    "        generation = generate_with_patch_layer(model, inp, layers)\n",
    "        result_generations.append({\n",
    "            'layers': [start_layer, end_layer],\n",
    "            'generation': generation\n",
    "        })\n",
    "    \n",
    "    return result_generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Pruning Experiment\n",
    "\n",
    "Performing layer pruning by intervening different layer groups (up to 1/4 of total layers).\n",
    "We test all possible consecutive layer interventions and evaluate harmful content generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def layer_pruning_experiment(harmful_data, max_samples=None):\n",
    "    \"\"\"\n",
    "    Perform layer pruning experiment on harmful prompts\n",
    "    \n",
    "    Args:\n",
    "        harmful_data: List of harmful behavior prompts\n",
    "        max_samples: Maximum number of samples to test (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        results_dict: Dictionary containing all experimental results\n",
    "    \"\"\"\n",
    "    model = mt.model\n",
    "    total_layers = model.config.num_hidden_layers\n",
    "    max_intervention_layers = total_layers // 4  # Maximum 1/4 of layers\n",
    "    \n",
    "    # Initialize result tracking\n",
    "    results_dict = {\n",
    "        'prompt_has_harmful': defaultdict(bool),  # Track if any intervention causes harmful output for each prompt\n",
    "        'layer_frequency': defaultdict(int),  # Track frequency of each layer appearing in harmful interventions\n",
    "        'detailed_results': []  # Store all detailed results\n",
    "    }\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    test_data = harmful_data[:max_samples] if max_samples else harmful_data\n",
    "    \n",
    "    print(f\"Total layers: {total_layers}\")\n",
    "    print(f\"Max intervention layers: {max_intervention_layers}\")\n",
    "    print(f\"Testing {len(test_data)} prompts\")\n",
    "    \n",
    "    # Iterate through each harmful prompt\n",
    "    for prompt_idx, prompt in enumerate(test_data):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Testing prompt {prompt_idx + 1}/{len(test_data)}: {prompt[:50]}...\")\n",
    "        \n",
    "        # Test different intervention sizes (2 to max_intervention_layers)\n",
    "        for n_layers in range(2, max_intervention_layers + 1):\n",
    "            print(f\"\\n  Intervention size: {n_layers} layers\")\n",
    "            \n",
    "            # Generate all possible consecutive layer combinations\n",
    "            layer_combinations = []\n",
    "            for start_layer in range(total_layers - n_layers + 1):\n",
    "                end_layer = start_layer + n_layers - 1\n",
    "                layer_combinations.append([start_layer, end_layer])\n",
    "            \n",
    "            print(f\"  Testing {len(layer_combinations)} layer combinations\")\n",
    "            \n",
    "            # Test each layer combination\n",
    "            for combo_idx, (start_layer, end_layer) in enumerate(layer_combinations):\n",
    "                try:\n",
    "                    # Generate input\n",
    "                    inp = make_inputs(mt.tokenizer, [prompt])\n",
    "                    \n",
    "                    # Get layer names\n",
    "                    layers = [layername(model, start_layer), layername(model, end_layer)]\n",
    "                    \n",
    "                    # Generate with layer intervention\n",
    "                    generation = generate_with_patch_layer(model, inp, layers)\n",
    "                    generation_text = generation[0] if isinstance(generation, list) else generation\n",
    "                    \n",
    "                    # Judge if output is harmful\n",
    "                    is_harmful = harmful_judge_from_strings(\n",
    "                                        prompt, \n",
    "                                        generation_text,\n",
    "                                        judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "                                    )\n",
    "                    \n",
    "                    # Store detailed result\n",
    "                    result_entry = {\n",
    "                        'prompt_idx': prompt_idx,\n",
    "                        'prompt': prompt,\n",
    "                        'n_layers': n_layers,\n",
    "                        'start_layer': start_layer,\n",
    "                        'end_layer': end_layer,\n",
    "                        'intervened_layers': list(range(start_layer, end_layer + 1)),\n",
    "                        'generation': generation_text,\n",
    "                        'is_harmful': is_harmful\n",
    "                    }\n",
    "                    results_dict['detailed_results'].append(result_entry)\n",
    "                    \n",
    "                    # Update tracking if harmful\n",
    "                    if is_harmful:\n",
    "                        results_dict['prompt_has_harmful'][prompt_idx] = True\n",
    "                        \n",
    "                        # Update layer frequency for all intervened layers\n",
    "                        for layer in range(start_layer, end_layer + 1):\n",
    "                            results_dict['layer_frequency'][layer] += 1\n",
    "                        \n",
    "                        print(f\"    ✓ Combination [{start_layer}-{end_layer}]: HARMFUL\")\n",
    "                    else:\n",
    "                        print(f\"    ✗ Combination [{start_layer}-{end_layer}]: Safe\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error processing combination [{start_layer}-{end_layer}]: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting layer pruning experiment...\")\n",
    "results = layer_pruning_experiment(harmful_data, max_samples=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"analysis_results/layer_intervention_results.json\",'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "Analyzing the experimental results to understand:\n",
    "1. Which prompts have at least one intervention that produces harmful output\n",
    "2. Which layers appear most frequently in harmful interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistics for prompts with harmful outputs\n",
    "total_prompts = len(set([r['prompt_idx'] for r in results['detailed_results']]))\n",
    "prompts_with_harmful = sum(results['prompt_has_harmful'].values())\n",
    "\n",
    "print(f\"\\n1. PROMPT-LEVEL STATISTICS:\")\n",
    "print(f\"   Total prompts tested: {total_prompts}\")\n",
    "print(f\"   Prompts with at least one harmful intervention: {prompts_with_harmful}\")\n",
    "print(f\"   Percentage: {prompts_with_harmful/total_prompts*100:.2f}%\")\n",
    "\n",
    "# Layer frequency analysis\n",
    "print(f\"\\n2. LAYER FREQUENCY IN HARMFUL INTERVENTIONS:\")\n",
    "if results['layer_frequency']:\n",
    "    sorted_layers = sorted(results['layer_frequency'].items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"   Top 10 most frequent layers:\")\n",
    "    for layer, freq in sorted_layers[:10]:\n",
    "        print(f\"      Layer {layer}: {freq} occurrences\")\n",
    "else:\n",
    "    print(\"   No harmful interventions found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results['layer_frequency']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    layers = sorted(results['layer_frequency'].keys())\n",
    "    frequencies = [results['layer_frequency'][l] for l in layers]\n",
    "    \n",
    "    plt.bar(layers, frequencies, color='steelblue', alpha=0.7)\n",
    "    plt.xlabel('Layer Index', fontsize=12)\n",
    "    plt.ylabel('Frequency in Harmful Interventions', fontsize=12)\n",
    "    plt.title('Layer Frequency Distribution in Harmful Interventions', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
