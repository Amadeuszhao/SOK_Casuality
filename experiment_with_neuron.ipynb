{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelUtils import *\n",
    "from utils.utils import *\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from casper import nethook\n",
    "from casper.nethook import *\n",
    "from utils.judgeUtils import *\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "Loading Llama-2-7b-chat model for neuron intervention analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    "    device = 'cuda:0'\n",
    ")\n",
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = generate_input(mt.tokenizer, 'tell me a fun joke')\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge Model Loading\n",
    "\n",
    "Loading the judge model to evaluate whether responses successfully jailbreak the safety mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading judge model...\")\n",
    "judge_tokenizer, judge_model_loaded = load_judge_model()\n",
    "print(\"Judge model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Loading benign and harmful prompts for neuron-level intervention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"analysis_data/llama2_gcgs.json\",'r') as f:\n",
    "    gcgs = json.load(f)[:3]\n",
    "with open(\"analysis_data/llama2_autodan.json\",'r') as f:\n",
    "    autodan = json.load(f)[:3]\n",
    "import pandas as pd\n",
    "harmful_data = pd.read_csv(\"analysis_data/advbench_behaviors.csv\")['Behavior']\n",
    "harmful_questions = list(harmful_data)[:100]\n",
    "with open(\"analysis_data/alpaca_clean.json\",'r') as f:\n",
    "    alpaca = json.load(f)\n",
    "safe_questions = [p['instruction'] for p in alpaca][:100]\n",
    "questions = harmful_questions + safe_questions\n",
    "labels = [1] * len(harmful_questions) + [0] * len(safe_questions)  # 1=harmful, 0=safe\n",
    "\n",
    "print(f\"Total questions: {len(questions)}\")\n",
    "print(f\"Harmful: {sum(labels)}, Safe: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [mt.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": q}], \n",
    "                                             tokenize=False, \n",
    "                                             add_generation_prompt=True)  for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "device = 'cuda'\n",
    "def collect_activations_nethook(mt, prompts, layer_names, batch_size=8):\n",
    "    \"\"\"\n",
    "    Collect activations using nethook.TraceDict.\n",
    "    Returns a dictionary mapping layer names to activation arrays.\n",
    "    \"\"\"\n",
    "    all_activations = {name: [] for name in layer_names}\n",
    "    \n",
    "    total_batches = (len(prompts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), total=total_batches, desc=\"Collecting activations\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        inputs = mt.tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with nethook.TraceDict(mt.model, layer_names, retain_output=True) as traces:\n",
    "                _ = mt.model(**inputs)\n",
    "                \n",
    "                # Extract activations for each layer\n",
    "                for layer_name in layer_names:\n",
    "                    output = traces[layer_name].output\n",
    "                    \n",
    "                    # Handle tuple outputs (some layers return tuples)\n",
    "                    if isinstance(output, tuple):\n",
    "                        output = output[0]\n",
    "                    \n",
    "                    # Take max over sequence dimension and move to CPU\n",
    "                    act = output.max(dim=1)[0].detach().cpu().float().numpy()\n",
    "                    all_activations[layer_name].append(act)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    for layer_name in layer_names:\n",
    "        all_activations[layer_name] = np.concatenate(all_activations[layer_name], axis=0)\n",
    "        print(f\"Layer {layer_name}: shape {all_activations[layer_name].shape}\")\n",
    "    \n",
    "    return all_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations using nethook\n",
    "target_layers = []\n",
    "for name, module in mt.model.named_modules():\n",
    "    if any(keyword in name.lower() for keyword in [\"gate\", \"up\"]):\n",
    "        target_layers.append(name)\n",
    "\n",
    "print(f\"Found {len(target_layers)} target layers\")\n",
    "print(f\"First 5 layers: {target_layers[:5]}\")\n",
    "\n",
    "activations = collect_activations_nethook(\n",
    "    mt,\n",
    "    prompts, \n",
    "    target_layers, \n",
    "    batch_size=8\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Toxic Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_probe(activations_tensor, labels_tensor, device='cuda:0', num_runs=1, lr=0.01, epochs=100):\n",
    "    \"\"\"\n",
    "    Train a linear probe to identify safety neurons.\n",
    "    Returns averaged weights across multiple runs.\n",
    "    \"\"\"\n",
    "    hidden_size = activations_tensor.shape[1]\n",
    "    all_weights = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        probe = torch.nn.Linear(hidden_size, 1).to(device)\n",
    "        optimizer = torch.optim.Adam(probe.parameters(), lr=lr)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = probe(activations_tensor)\n",
    "            loss = criterion(outputs, labels_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        weights = probe.weight.detach().cpu().numpy().flatten()\n",
    "        all_weights.append(weights)\n",
    "    \n",
    "    # Average weights across runs\n",
    "    avg_weights = np.mean(all_weights, axis=0)\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_neuron_threshold = 3\n",
    "toxic_neurons_dict = {}\n",
    "weights_dict = {}\n",
    "\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    "\n",
    "print(\"\\nComputing toxic neurons for each layer...\")\n",
    "for layer_name, act_matrix in tqdm(activations.items(), desc=\"Processing layers\"):\n",
    "    \n",
    "    activations_tensor = torch.tensor(act_matrix, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    assert activations_tensor.shape[0] == labels_tensor.shape[0], \\\n",
    "        f\"Shape mismatch: activations {activations_tensor.shape[0]} vs labels {labels_tensor.shape[0]}\"\n",
    "    \n",
    "    # Train safety probe\n",
    "    weights = safety_probe(activations_tensor, labels_tensor, device=device, num_runs=1)\n",
    "    weights_dict[layer_name] = weights\n",
    "    \n",
    "    # Identify toxic neurons using z-score threshold\n",
    "    z_scores = zscore(weights)\n",
    "    toxic_neurons = np.where((np.abs(z_scores) > safety_neuron_threshold) & (weights > 0))[0]\n",
    "    toxic_neurons_dict[layer_name] = toxic_neurons\n",
    "    \n",
    "\n",
    "total_toxic = sum(len(neurons) for neurons in toxic_neurons_dict.values())\n",
    "print(f\"\\nTotal toxic neurons across all layers: {total_toxic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic Neuron Intervention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_neuron_intervention(\n",
    "    mt,\n",
    "    prompt,\n",
    "    toxic_neurons_dict,\n",
    "    intervene=True,\n",
    "    max_new_tokens=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text with toxic neurons zeroed out using forward hooks.\n",
    "    \"\"\"\n",
    "    if hasattr(mt.tokenizer, 'apply_chat_template'):\n",
    "        formatted_prompt = mt.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    inputs = mt.tokenizer(formatted_prompt, return_tensors=\"pt\").to(mt.model.device)\n",
    "    \n",
    "    if not intervene:\n",
    "        # Generate without intervention (baseline)\n",
    "        with torch.no_grad():\n",
    "            outputs = mt.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=mt.tokenizer.pad_token_id\n",
    "            )\n",
    "        generation = mt.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return generation\n",
    "    \n",
    "    # Create intervention hook\n",
    "    def make_intervention_hook(layer_name):\n",
    "        def hook(module, input, output):\n",
    "            if layer_name not in toxic_neurons_dict:\n",
    "                return output\n",
    "            \n",
    "            # Handle both tensor and tuple outputs\n",
    "            if isinstance(output, tuple):\n",
    "                result = list(output)\n",
    "                result[0] = result[0].clone()\n",
    "            else:\n",
    "                result = output.clone()\n",
    "            \n",
    "            toxic_neurons = toxic_neurons_dict[layer_name]\n",
    "            \n",
    "            if len(toxic_neurons) > 0:\n",
    "                if isinstance(result, list):\n",
    "                    result[0][..., toxic_neurons] = 0\n",
    "                    return tuple(result)\n",
    "                else:\n",
    "                    result[..., toxic_neurons] = 0\n",
    "                    return result\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for layer_name in toxic_neurons_dict.keys():\n",
    "        # Get the module\n",
    "        module = mt.model.get_submodule(layer_name)\n",
    "        hook_handle = module.register_forward_hook(make_intervention_hook(layer_name))\n",
    "        hooks.append(hook_handle)\n",
    "    \n",
    "    try:\n",
    "        # Generate with intervention\n",
    "        with torch.no_grad():\n",
    "            outputs = mt.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=mt.tokenizer.pad_token_id\n",
    "            )\n",
    "        generation = mt.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for hook_handle in hooks:\n",
    "            hook_handle.remove()\n",
    "    \n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_harmful_data = pd.read_csv(\"analysis_data/advbench_behaviors.csv\")['Behavior']\n",
    "test_prompts = list(test_harmful_data)[100:150]  # Use different prompts for testing\n",
    "\n",
    "print(f\"Testing on {len(test_prompts)} harmful prompts...\")\n",
    "\n",
    "results = {\n",
    "    'baseline': {'harmful': 0, 'total': 0},\n",
    "    'intervened': {'harmful': 0, 'total': 0},\n",
    "    'detailed_results': []\n",
    "}\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(test_prompts, desc=\"Testing prompts\")):\n",
    "    try:\n",
    "        # Generate baseline (no intervention)\n",
    "        baseline_gen = generate_with_neuron_intervention(\n",
    "            mt, prompt, toxic_neurons_dict, intervene=False, max_new_tokens=50\n",
    "        )\n",
    "        \n",
    "        # Generate with intervention\n",
    "        intervened_gen = generate_with_neuron_intervention(\n",
    "            mt, prompt, toxic_neurons_dict, intervene=True, max_new_tokens=50\n",
    "        )\n",
    "        \n",
    "        # Judge both outputs\n",
    "        baseline_harmful = harmful_judge_from_strings(\n",
    "            prompt, baseline_gen, judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "        )\n",
    "        intervened_harmful = harmful_judge_from_strings(\n",
    "            prompt, intervened_gen, judge_model=(judge_tokenizer, judge_model_loaded)\n",
    "        )\n",
    "        \n",
    "        # Update statistics\n",
    "        results['baseline']['total'] += 1\n",
    "        results['intervened']['total'] += 1\n",
    "        \n",
    "        if baseline_harmful:\n",
    "            results['baseline']['harmful'] += 1\n",
    "        if intervened_harmful:\n",
    "            results['intervened']['harmful'] += 1\n",
    "        \n",
    "        # Store detailed result\n",
    "        results['detailed_results'].append({\n",
    "            'prompt': prompt,\n",
    "            'baseline_generation': baseline_gen,\n",
    "            'baseline_harmful': baseline_harmful,\n",
    "            'intervened_generation': intervened_gen,\n",
    "            'intervened_harmful': intervened_harmful,\n",
    "            'intervention_successful': baseline_harmful and not intervened_harmful\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on prompt {idx}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Calculate success rates\n",
    "baseline_jailbreak_rate = results['baseline']['harmful'] / results['baseline']['total']\n",
    "intervened_jailbreak_rate = results['intervened']['harmful'] / results['intervened']['total']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEURON INTERVENTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBaseline (No Intervention):\")\n",
    "print(f\"  Harmful outputs: {results['baseline']['harmful']}/{results['baseline']['total']}\")\n",
    "print(f\"  Jailbreak rate: {baseline_jailbreak_rate*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nWith Neuron Intervention:\")\n",
    "print(f\"  Harmful outputs: {results['intervened']['harmful']}/{results['intervened']['total']}\")\n",
    "print(f\"  Jailbreak rate: {intervened_jailbreak_rate*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nIntervention Effectiveness:\")\n",
    "print(f\"  Reduction in jailbreak rate: {(baseline_jailbreak_rate - intervened_jailbreak_rate)*100:.2f}%\")\n",
    "print(f\"  Total toxic neurons intervened: {total_toxic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"analysis_results/neuron_intervention_results.json\",'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Toxic Neuron Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_toxic_neuron_positions(toxic_neurons_dict):\n",
    "    \"\"\"Visualize toxic neuron positions across layers.\"\"\"\n",
    "    layer_data = []\n",
    "    for layer_name, neuron_indices in toxic_neurons_dict.items():\n",
    "        # Extract layer number\n",
    "        layer_num = None\n",
    "        parts = layer_name.split('.')\n",
    "        for i, part in enumerate(parts):\n",
    "            if part.isdigit():\n",
    "                layer_num = int(part)\n",
    "                break\n",
    "        \n",
    "        if layer_num is not None:\n",
    "            for neuron_idx in neuron_indices:\n",
    "                layer_data.append({\n",
    "                    'layer': layer_num,\n",
    "                    'neuron': neuron_idx,\n",
    "                    'layer_name': layer_name\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(layer_data)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Scatter plot of neuron positions\n",
    "    ax1.scatter(df['layer'], df['neuron'], alpha=0.6, s=20, c='red')\n",
    "    ax1.set_xlabel('Layer Index', fontsize=12)\n",
    "    ax1.set_ylabel('Neuron Index', fontsize=12)\n",
    "    ax1.set_title('Toxic Neuron Positions Across Layers', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of toxic neurons per layer\n",
    "    layer_counts = df.groupby('layer').size()\n",
    "    ax2.bar(layer_counts.index, layer_counts.values, color='steelblue', alpha=0.7)\n",
    "    ax2.set_xlabel('Layer Index', fontsize=12)\n",
    "    ax2.set_ylabel('Number of Toxic Neurons', fontsize=12)\n",
    "    ax2.set_title('Distribution of Toxic Neurons Across Layers', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"toxic_neuron_positions.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOXIC NEURON POSITION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal toxic neurons: {len(df)}\")\n",
    "    print(f\"Layers with toxic neurons: {df['layer'].nunique()}\")\n",
    "    print(f\"Average toxic neurons per layer: {len(df) / df['layer'].nunique():.2f}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 layers with most toxic neurons:\")\n",
    "    for layer, count in layer_counts.nlargest(5).items():\n",
    "        print(f\"  Layer {layer}: {count} toxic neurons\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_neuron_df = visualize_toxic_neuron_positions(toxic_neurons_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
